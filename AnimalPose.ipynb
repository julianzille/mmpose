{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "732b4e61-00c0-472f-b3f7-4e84f05ee916",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be607bb-153a-48cb-ae84-bce00c00f97f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install -U openmim\n",
    "!mim install mmcv-full\n",
    "\n",
    "# install mmpose dependencies\n",
    "%pip install -v -r requirements.txt\n",
    "\n",
    "%pip install mmdet\n",
    "%pip install mmpose\n",
    "# install mmpose in develop mode\n",
    "%pip install -v -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0313b193-5ee9-4277-8896-4f03cf45e102",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mmcv\n",
    "from mmcv import Config, DictAction\n",
    "import mmpose\n",
    "import torch, torchvision\n",
    "import time\n",
    "import os\n",
    "import shutil\n",
    "import os.path as osp\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "from collections import OrderedDict, defaultdict\n",
    "import torch.distributed as dist\n",
    "from mmcv.runner import get_dist_info, init_dist, set_random_seed\n",
    "from mmcv.utils import get_git_hash\n",
    "from mmpose import __version__\n",
    "from mmpose.apis import init_random_seed, train_model\n",
    "from mmpose.models import build_posenet\n",
    "from mmpose.utils import collect_env, get_root_logger, setup_multi_processes\n",
    "\n",
    "from mmcv.cnn import fuse_conv_bn\n",
    "from mmcv.parallel import MMDataParallel, MMDistributedDataParallel\n",
    "from mmcv.runner import get_dist_info, init_dist, load_checkpoint\n",
    "\n",
    "from mmpose.apis import multi_gpu_test, single_gpu_test\n",
    "from mmpose.datasets import build_dataloader, build_dataset\n",
    "\n",
    "try:\n",
    "    from mmcv.runner import wrap_fp16_model\n",
    "except ImportError:\n",
    "    warnings.warn('auto_fp16 from mmpose will be deprecated from v0.15.0'\n",
    "                  'Please install mmcv>=1.1.4')\n",
    "    from mmpose.core import wrap_fp16_model\n",
    "    \n",
    "from matplotlib import pprint\n",
    "from mmpose.apis import (inference_top_down_pose_model, init_pose_model,\n",
    "                         vis_pose_result, process_mmdet_results)\n",
    "from mmdet.apis import inference_detector, init_detector\n",
    "from xtcocotools.coco import COCO\n",
    "\n",
    "local_runtime = False\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from mmpose.core.bbox.transforms import bbox_xywh2xyxy\n",
    "from mmpose.datasets import DatasetInfo\n",
    "import matplotlib\n",
    "from mmpose.apis import (collect_multi_frames, get_track_id,\n",
    "                         inference_top_down_pose_model, init_pose_model,\n",
    "                         process_mmdet_results, vis_pose_tracking_result)\n",
    "\n",
    "from mmpose.core import Smoother\n",
    "from mmpose.apis import (collect_multi_frames, extract_pose_sequence,\n",
    "                         get_track_id, inference_pose_lifter_model,\n",
    "                         inference_top_down_pose_model, init_pose_model,\n",
    "                         process_mmdet_results, vis_3d_pose_result)\n",
    "\n",
    "from mmpose.core import Smoother\n",
    "from mmpose.datasets import DatasetInfo\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b701fbaf-6140-438d-9e58-69d83f5d736b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007952e6-49ea-4aa7-8b3c-48c9eea15947",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Extract all AcinoSet 2D Labelled data: COCO format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af35509f-ff5a-4e95-8916-ec0257cd31da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NB: unzip \"labelled_data.zip\" to \"/notebooks/data/acino/labelled\" first\n",
    "\n",
    "acino='/notebooks/data/acino'\n",
    "\n",
    "img_ids=[os.path.splitext(img)[0] for img in os.listdir(acino+'/../ap10k/data/')]\n",
    "img_ids.remove('.ipynb_checkpoints')\n",
    "img_ids=[int(i) for i in img_ids]\n",
    "img_no=max(img_ids)+1\n",
    "\n",
    "base=pd.read_csv('base.csv')\n",
    "\n",
    "for dir in os.listdir(acino+'/labelled'): #JamesFlick1 JamesFlick2 etc.\n",
    "    print(dir)\n",
    "    if os.path.isdir(acino+'/labelled/'+dir):\n",
    "        \n",
    "        df=pd.read_csv('labelled/'+dir+'/'+\"CollectedData_UCT.csv\")\n",
    "\n",
    "        df.iloc[0,1:]=df.iloc[0,1:].astype(str) +'_'+ df.iloc[1,1:].astype(str)\n",
    "        df.columns=df.iloc[0] # Make keypoint labels column headers\n",
    "        df=df.iloc[2:,:]\n",
    "        \n",
    "        #Extract COCO keypoints (AP10K):\n",
    "        #df=df.reindex(columns=(['bodyparts','l_eye_x','l_eye_y','r_eye_x','r_eye_y','nose_x','nose_y','neck_base_x','neck_base_y','tail_base_x','tail_base_y','l_shoulder_x','l_shoulder_y','l_front_knee_x','l_front_knee_y','l_front_paw_x','l_front_paw_y','r_shoulder_x','r_shoulder_y','r_front_knee_x','r_front_knee_y','r_front_paw_x','r_front_paw_y','l_hip_x','l_hip_y','l_back_knee_x','l_back_knee_y','l_back_paw_x','l_back_paw_y','r_hip_x','r_hip_y','r_back_knee_x','r_back_knee_y','r_back_paw_x','r_back_paw_y','tail2_x','tail2_y']))\n",
    "        \n",
    "        #Extract COCO keypoints (All Acino Keypoints):\n",
    "        df=df.reindex(columns=(['bodyparts','l_eye_x','l_eye_y','r_eye_x','r_eye_y','nose_x','nose_y','neck_base_x','neck_base_y','tail_base_x','tail_base_y','l_shoulder_x','l_shoulder_y','l_front_knee_x','l_front_knee_y','l_front_paw_x','l_front_paw_y','r_shoulder_x','r_shoulder_y','r_front_knee_x','r_front_knee_y','r_front_paw_x','r_front_paw_y','l_hip_x','l_hip_y','l_back_knee_x','l_back_knee_y','l_back_paw_x','l_back_paw_y','r_hip_x','r_hip_y','r_back_knee_x','r_back_knee_y','r_back_paw_x','r_back_paw_y','tail2_x','tail2_y','tail1_x','tail1_y','r_front_ankle_x','r_front_ankle_y','l_front_ankle_x','l_front_ankle_y','r_back_ankle_x','r_back_ankle_y','l_back_ankle_x','l_back_ankle_y','spine_x','spine_y']))\n",
    "        \n",
    "        for i in range(0,len(df)): # Iterate over rows\n",
    "            orig=df.iloc[i,0].split('/')[2]\n",
    "            df.iloc[i,0]='as_'+str(img_no)+'.jpg'\n",
    "            \n",
    "            im=Image.open('data/acino/labelled/'+dir+'/'+orig)\n",
    "            img_name='as_'+str(img_no)\n",
    "            im.save('data/acino/data/'+img_name+'.jpg')\n",
    "            \n",
    "            #os.remove('labelled1/'+dir+'/'+orig)\n",
    "            \n",
    "            \n",
    "            img_no+=1\n",
    "        base=pd.concat([base,df[0:]],ignore_index=True)\n",
    "               \n",
    "base.to_csv(\"data/acino/annotations.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33d8406-440c-4693-a9be-107e22cf6112",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/acino/annotations.csv')\n",
    "\n",
    "if os.path.exists(\"data/acino/annotations/acino_all.json\"):\n",
    "    os.remove(\"data/acino/annotations/acino_all.json\")\n",
    "    \n",
    "shutil.copyfile('data/acino/annotations/anns_empty_acino.json','data/acino/annotations/acino_all.json')\n",
    "\n",
    "with open('data/acino/annotations/acino_all.json','r+') as f: # Save all annotation instances in one .json file\n",
    "    anns_data=json.load(f)\n",
    "    \n",
    "    for i in range(0,len(df)):# Iterate over each image\n",
    "\n",
    "        #Add image instance\n",
    "        img_name=df.iloc[i,0]\n",
    "        img_id=int(img_name[3:].split('.')[0])\n",
    "        img=cv2.imread('data/acino/data/'+img_name)\n",
    "        height = img.shape[0]\n",
    "        width = img.shape[1]\n",
    "        img_inst=dict(width=width,height=height,file_name=img_name,background=1,id=img_id)\n",
    "        \n",
    "        #Add annotation instance\n",
    "        ann_id=img_id\n",
    "        kp=[]\n",
    "        num_kp=0\n",
    "\n",
    "        # Initial bbox:\n",
    "        x_min=width-1\n",
    "        x_max=1\n",
    "        y_min=height-1\n",
    "        y_max=1\n",
    "\n",
    "        for j in range(1,len(df.columns),2): # -12 for AP10k format\n",
    "            if pd.isnull(df.iloc[i,j]): #NaN values\n",
    "                kp.append(0)\n",
    "                kp.append(0)\n",
    "                kp.append(0)\n",
    "                \n",
    "            else:\n",
    "                x=int(round(pd.to_numeric(df.iloc[i,j])))\n",
    "                y=int(round(pd.to_numeric(df.iloc[i,j+1])))\n",
    "                \n",
    "                #Add annotations\n",
    "                kp.append(x)\n",
    "                kp.append(y)\n",
    "                kp.append(2)\n",
    "                num_kp+=1\n",
    "\n",
    "                #Bbox\n",
    "                if x<x_min:\n",
    "                    x_min=x\n",
    "                if x>x_max:\n",
    "                    x_max=x\n",
    "                if y>y_max:\n",
    "                    y_max=y\n",
    "                if y<y_min:\n",
    "                    y_min=y\n",
    "                    \n",
    "        # Pad bbox with extra pixels:\n",
    "        x_min=x_min-15\n",
    "        y_min=y_min-15\n",
    "        \n",
    "        # BBox w, h\n",
    "        w=max(1,(x_max-x_min))+30\n",
    "        h=max(1,(y_max-y_min))+30\n",
    "        a=w*h # Bbox area\n",
    "        \n",
    "        #Padded bounding box:\n",
    "        bbox=[x_min,y_min,w,h] # xywh format NB! y_min==top left corner\n",
    "        \n",
    "        # Uncomment for AP10k format:\n",
    "        \n",
    "#         if kp[-1]==2: # If tip of tail != NaN\n",
    "#             num_kp-=1 \n",
    "            \n",
    "#         del kp[-3:] # Remove tail_x, tail_y, visible\n",
    "        \n",
    "        \n",
    "        ann_inst=dict(image_id=img_id,iscrowd=0,category_id=1,num_keypoints=num_kp,keypoints=kp,bbox=bbox,id=ann_id,area=a)\n",
    "        anns_data['images'].append(img_inst)\n",
    "        anns_data['annotations'].append(ann_inst)\n",
    "    f.seek(0)\n",
    "    json.dump(anns_data,f,indent=4)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4173c2-ac9d-432c-9dad-c862f49741d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install echo1-coco-split\n",
    "\n",
    "!coco-split --annotations_file data/acino/annotations/acino_all.json --valid_ratio 0.1 --test_ratio 0.2 --train_name data/acino/annotations/acino_train.json --valid_name data/acino/annotations/acino_val.json --test_name data/acino/annotations/acino_test.json --has_annotations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "257e36c3-67ee-407b-be32-ddf9e6458ced",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train, test and demo 2D pose estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4af1a2-f47a-480c-b53e-031383188378",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05634697-c9a7-4cd3-8228-4fa39376590a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Config file:\n",
    "cfg = Config.fromfile('configs/animal/2d_kpt_sview_rgb_img/topdown_heatmap/acino/res50_acino_256x256.py')\n",
    "# For new configs:\n",
    "\n",
    "# wrkdir = os.path.join('./work_dirs',\n",
    "                                # os.path.splitext(os.path.basename('configs/animal/2d_kpt_sview_rgb_img/topdown_heatmap/acino/hrnet_w32_acino_256x256.py'))[0]+'_'+time.strftime('%d-%m_', time.localtime())+\"ep\"+str(cfg.total_epochs))\n",
    "\n",
    "# if (os.path.isdir(wrkdir)):\n",
    "#     cfg.\n",
    "# else:\n",
    "#     cfg.work_dir=wrkdir\n",
    "cfg.work_dir='work_dirs/res50_acino_256x256_epoch150'    \n",
    "# create work_dir\n",
    "mmcv.mkdir_or_exist(cfg.work_dir)\n",
    "                                  \n",
    "# Config Parameters:\n",
    "cfg.total_epochs=150\n",
    "\n",
    "cfg.resume_from=None # the checkpoint file to resume from\n",
    "\n",
    "cfg.gpu_ids=range(1)\n",
    "cfg.workflow=[('train',1)]\n",
    "\n",
    "cfg.log_name = time.strftime('%d-%m-', time.localtime())+\"ep\"+str(cfg.total_epochs)\n",
    "cfg.log_file = os.path.join(cfg.work_dir, f'{cfg.log_name}.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918aa8bb-60b0-41a0-95a8-cb886f4e96d8",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Train model from cfg: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea8d327-b28f-46da-a97e-4e9afe935862",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train from config file\n",
    "\n",
    "autoscale_lr = False # automatically scale lr with the number of gpus\n",
    "launcher = 'none' # Job launcher. ['none', 'pytorch', 'slurm', 'mpi']\n",
    "deterministic = False # Whether to set deterministic options for CUDNN backend.\n",
    "diff_seed = False # Whether or not set different seeds for different ranks\n",
    "seed = 0 \n",
    "\n",
    "if autoscale_lr:\n",
    "    # apply the linear scaling rule (https://arxiv.org/abs/1706.02677)\n",
    "    cfg.optimizer['lr'] = cfg.optimizer['lr'] * len(cfg.gpu_ids) / 8\n",
    "\n",
    "if launcher == 'none':\n",
    "    distributed = False\n",
    "    if len(cfg.gpu_ids) > 1:\n",
    "        warnings.warn(\n",
    "            f'We treat {cfg.gpu_ids} as gpu-ids, and reset to '\n",
    "            f'{cfg.gpu_ids[0:1]} as gpu-ids to avoid potential error in '\n",
    "            'non-distribute training time.')\n",
    "        cfg.gpu_ids = cfg.gpu_ids[0:1]\n",
    "else:\n",
    "    distributed = True\n",
    "    init_dist(launcher, **cfg.dist_params)\n",
    "    # re-set gpu_ids with distributed training mode\n",
    "    _, world_size = get_dist_info()\n",
    "    cfg.gpu_ids = range(world_size)\n",
    "\n",
    "# set multi-process settings\n",
    "setup_multi_processes(cfg)\n",
    "\n",
    "# init the logger before other steps\n",
    "logger = get_root_logger(log_file=cfg.log_file, log_level=cfg.log_level)\n",
    "\n",
    "# init the meta dict to record some important information such as\n",
    "# environment info and seed, which will be logged\n",
    "meta = dict()\n",
    "\n",
    "# log env info\n",
    "env_info_dict = collect_env()\n",
    "env_info = '\\n'.join([(f'{k}: {v}') for k, v in env_info_dict.items()])\n",
    "dash_line = '-' * 60 + '\\n'\n",
    "# logger.info('Environment info:\\n' + dash_line + env_info + '\\n' +\n",
    "#             dash_line)\n",
    "meta['env_info'] = env_info\n",
    "\n",
    "# log some basic info\n",
    "logger.info(f'Distributed training: {distributed}')\n",
    "# logger.info(f'Config:\\n{cfg.pretty_text}')\n",
    "\n",
    "# set random seeds\n",
    "seed = init_random_seed(seed)\n",
    "seed = seed + dist.get_rank() if diff_seed else seed\n",
    "logger.info(f'Set random seed to {seed}, '\n",
    "            f'deterministic: {deterministic}')\n",
    "set_random_seed(seed, deterministic=deterministic)\n",
    "cfg.seed = seed\n",
    "meta['seed'] = seed\n",
    "\n",
    "# build dataset\n",
    "train_set = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# build model\n",
    "model = build_posenet(cfg.model)\n",
    "\n",
    "if len(cfg.workflow) == 2:\n",
    "    val_dataset = copy.deepcopy(cfg.data.val)\n",
    "    val_dataset.pipeline = cfg.data.train.pipeline\n",
    "    train_set.append(build_dataset(val_dataset))\n",
    "\n",
    "if cfg.checkpoint_config is not None:\n",
    "    # save mmpose version, config file content\n",
    "    # checkpoints as meta data\n",
    "    cfg.checkpoint_config.meta = dict(\n",
    "        mmpose_version=__version__ + get_git_hash(digits=7),\n",
    "        # config=cfg.pretty_text,\n",
    "    )\n",
    "\n",
    "timestamp = time.strftime('%d-%m-', time.localtime())+\"ep\"+str(cfg.total_epochs)\n",
    "print(\"LINE\")\n",
    "# train model\n",
    "train_model(\n",
    "    model,\n",
    "    train_set, \n",
    "    cfg, \n",
    "    distributed=distributed, \n",
    "    validate=True, \n",
    "    timestamp=timestamp,\n",
    "    meta=meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc852389-7870-4194-a910-f7f8bcf571e8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "print(cfg.pretty_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c3023a-5585-4587-9c06-9ab3f06952cf",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Test 2D pose estimator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8578c493",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "launcher='none' #job launcher ['none', 'pytorch', 'slurm', 'mpi']\n",
    "checkpoint=cfg.work_dir+'/latest.pth'\n",
    "evaluate='mAP' #evaluation metric, which depends on the dataset, \"mAP\"\n",
    "fuse_convbn=True\n",
    "gpu_id=cfg.gpu_ids[0] #only applicable to non-distributed testing\n",
    "gpu_collect=True #whether to use gpu to collect results\n",
    "local_rank=0\n",
    "tmpdir=None\n",
    "    \n",
    "# set multi-process settings\n",
    "setup_multi_processes(cfg)\n",
    "\n",
    "# set cudnn_benchmark\n",
    "if cfg.get('cudnn_benchmark', False):\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "cfg.model.pretrained = None\n",
    "# cfg.data.test.test_mode = True\n",
    "\n",
    "# init distributed env first, since logger depends on the dist info.\n",
    "if launcher == 'none':\n",
    "    distributed = False\n",
    "else:\n",
    "    distributed = True\n",
    "    init_dist(launcher, **cfg.dist_params)\n",
    "\n",
    "# build the dataloader\n",
    "test_set = build_dataset(cfg.data.test, dict(test_mode=True))\n",
    "\n",
    "# step 1: give default values and override (if exist) from cfg.data\n",
    "loader_cfg = {\n",
    "    **dict(seed=cfg.get('seed'), drop_last=False, dist=distributed),\n",
    "    **({} if torch.__version__ != 'parrots' else dict(\n",
    "           prefetch_num=2,\n",
    "           pin_memory=False,\n",
    "       )),\n",
    "    **dict((k, cfg.data[k]) for k in [\n",
    "               'seed',\n",
    "               'prefetch_num',\n",
    "               'pin_memory',\n",
    "               'persistent_workers',\n",
    "           ] if k in cfg.data)\n",
    "}\n",
    "# step2: cfg.data.test_dataloader has higher priority\n",
    "test_loader_cfg = {\n",
    "    **loader_cfg,\n",
    "    **dict(shuffle=False, drop_last=False),\n",
    "    **dict(workers_per_gpu=cfg.data.get('workers_per_gpu', 1)),\n",
    "    **dict(samples_per_gpu=cfg.data.get('samples_per_gpu', 1)),\n",
    "    **cfg.data.get('test_dataloader', {})\n",
    "}\n",
    "\n",
    "data_loader = build_dataloader(test_set, **test_loader_cfg)\n",
    "\n",
    "\n",
    "\n",
    "# build the model and load checkpoint\n",
    "model = build_posenet(cfg.model)\n",
    "fp16_cfg = cfg.get('fp16', None)\n",
    "if fp16_cfg is not None:\n",
    "    wrap_fp16_model(model)\n",
    "\n",
    "load_checkpoint(model, checkpoint, map_location='cpu')   \n",
    "if fuse_convbn:\n",
    "    model = fuse_conv_bn(model)\n",
    "if not distributed:\n",
    "    model = MMDataParallel(model, device_ids=[gpu_id])\n",
    "    outputs = single_gpu_test(model, data_loader)\n",
    "else:\n",
    "    model = MMDistributedDataParallel(\n",
    "        model.cuda(),\n",
    "        device_ids=[torch.cuda.current_device()],\n",
    "        broadcast_buffers=False)\n",
    "    outputs = multi_gpu_test(model, data_loader, tmpdir,\n",
    "                             gpu_collect)\n",
    "    \n",
    "\n",
    "rank, _ = get_dist_info()\n",
    "eval_config = cfg.get('evaluation', {})\n",
    "# eval_config = merge_configs(eval_config, dict(metric=evaluate))\n",
    "\n",
    "out_file = cfg.work_dir+'/test_results.json'\n",
    "\n",
    "if rank == 0:\n",
    "    if out_file:\n",
    "        print(f'\\nwriting results to {out_file}') \n",
    "        mmcv.dump(outputs, out_file)\n",
    "\n",
    "    results = test_set.evaluate(outputs, cfg.work_dir, **eval_config) # mmpose/datasets/datasets/animal/... evaluate()\n",
    "    for k, v in sorted(results.items()):\n",
    "        print(f'{k}: {v}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71733a8-9c4d-4c91-ab7d-e3630cdecd96",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 2D Estimator inference, analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde6f96c-1096-4c35-90c7-ccbbbea6ceb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test model:\n",
    "\n",
    "pose_checkpoint = cfg.work_dir+'/latest.pth'\n",
    "json_file_gt='data/acino/annotations/acino_all.json'\n",
    "\n",
    "# with open('./work_dirs/hrnet_w32_acino-10k_256x256_epoch50/result_keypoints.json','r') as f: \n",
    "#     json_file_pred=json.load(f)\n",
    "# f.close()\n",
    "\n",
    "# print()\n",
    "\n",
    "coco = COCO(json_file_gt)\n",
    "# print(coco)\n",
    "img_keys = list(coco.imgs.keys())\n",
    "# print(img_keys)\n",
    "# # initialize pose model\n",
    "pose_model = init_pose_model(cfg, pose_checkpoint)\n",
    "dataset = pose_model.cfg.data.test.type\n",
    "dataset_info = pose_model.cfg.data.test.get('dataset_info', None)\n",
    "dataset_info = DatasetInfo(dataset_info)\n",
    "return_heatmap=True\n",
    "output_layer_names=None\n",
    "\n",
    "n=1 # Num imgs to plot\n",
    "ntest=len(img_keys)\n",
    "show= np.random.choice(ntest,n)\n",
    "\n",
    "#GT:\n",
    "for i in range(n):\n",
    "    img_id=img_keys[show[i]]\n",
    "    image=coco.loadImgs(img_id)[0]\n",
    "    img_name=os.path.join('data/acino/data',image['file_name'])\n",
    "    \n",
    "    # pred=next(inst for inst in json_file_pred if inst['image_id']==img_id)\n",
    "    \n",
    "    \n",
    "    ann_ids=coco.getAnnIds(img_id)\n",
    "    # print(ann_ids)\n",
    "    ann=coco.anns[ann_ids[0]]\n",
    "    # print(ann)\n",
    "    animal_bbox=[]\n",
    "    gt_keypoints=[]\n",
    "    \n",
    "    for ann_id in ann_ids:\n",
    "        animal={}  \n",
    "        ann=coco.anns[ann_id]\n",
    "        animal['bbox']=ann['bbox']\n",
    "        animal_bbox.append(animal)\n",
    "        \n",
    "        gt={}\n",
    "        x=ann['bbox'][0]\n",
    "        y=ann['bbox'][1]\n",
    "        w=ann['bbox'][2]\n",
    "        h=ann['bbox'][3]\n",
    "        gt['bbox']=[x,y,x+w,y+h]\n",
    "        gt['keypoints']=np.array(ann['keypoints']).reshape(24,3)\n",
    "        gt_keypoints.append(gt)\n",
    "    \n",
    "    # x=ann['bbox'][0]\n",
    "    # y=ann['bbox'][1]\n",
    "    # w=ann['bbox'][2]\n",
    "    # h=ann['bbox'][3]\n",
    "#     print(ann['bbox'])\n",
    "    # bbox=[x,y,x+w,y+h]\n",
    "    \n",
    "#     print(ann['bbox'])\n",
    "\n",
    "    pose_results, returned_outputs = inference_top_down_pose_model(\n",
    "        pose_model,\n",
    "        img_name,\n",
    "        person_results=animal_bbox, # BBoxs required for top down\n",
    "        bbox_thr=None,\n",
    "        format='xywh',\n",
    "        dataset=dataset,\n",
    "        dataset_info=dataset_info,\n",
    "        return_heatmap=True, #for now\n",
    "        outputs=output_layer_names)\n",
    "    \n",
    "    out_file_pred = os.path.join('vis_result', f'vis_as{img_id}.jpg')\n",
    "    out_file_gt = os.path.join('vis_result', f'vis_gt_as{img_id}.jpg')\n",
    "    vis_result_pred=vis_pose_result(pose_model,img_name,pose_results,radius=3,thickness=1,out_file=out_file_pred)\n",
    "    # vis_result_pred = cv2.resize(vis_result_pred, dsize=None, fx=1.5, fy=1.5)\n",
    "    \n",
    "    vis_result_gt=vis_pose_result(pose_model,img_name,gt_keypoints,radius=3,thickness=1,out_file=out_file_gt)\n",
    "    # vis_result_gt = cv2.resize(vis_result_gt, dsize=None, fx=1.5, fy=1.5)\n",
    "    # vis_result_gt = vis_result_gt\n",
    "\n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(vis_result_gt[y-25:y+h+25,x-25:x+w+25])\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(15,10))\n",
    "    plt.imshow(vis_result_pred[y-25:y+h+50,x-25:x+w+50])\n",
    "    plt.show()\n",
    "    \n",
    "#     os.makedirs('vis_result', exist_ok=True)\n",
    "#     out_file = os.path.join('vis_result', f'vis_as{image_id}.jpg')\n",
    "#     vis_result=vis_pose_result(pose_model,image_name,pose_results,radius=3,thickness=1,out_file=out_file)\n",
    "#     vis_result = cv2.resize(vis_result, dsize=None, fx=1.5, fy=1.5)\n",
    "#     # plt.figure(figsize=(15,10))\n",
    "#     # plt.imshow(vis_result)\n",
    "#     # plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b1fc24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python tools/analysis/analyze_logs.py \\\n",
    "        plot_curve work_dirs/hrnet_w32_acino_256x256_epoch50/None.log.json \\\n",
    "        --keys acc_pose \\\n",
    "        --legend Accuracy \\\n",
    "        --out acino_accuracy.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61234feb-2dc2-48d7-8eca-728b39a8b5d6",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### 2D Estimator demo with MMDet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e8eb10-58ba-4800-a3a6-1f419089811e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    " \n",
    "det_config='MMDet/configs/faster_rcnn_r50_fpn_2x_acino.py'\n",
    "det_checkpoint='MMDet/epoch_4.pth'\n",
    "\n",
    "pose_config='configs/animal/2d_kpt_sview_rgb_img/topdown_heatmap/acino/res50_acino_256x256.py'\n",
    "pose_checkpoint='work_dirs/res50_acino_256x256_epoch150/latest.pth'\n",
    "\n",
    "vid='menyacam3.mp4'#'../storage/cam3.mp4'\n",
    "out_vid='work_dirs/res50_acino_256x256_epoch150/'\n",
    "use_multi_frames=False\n",
    "online=True\n",
    "thickness=1\n",
    "radius=4\n",
    "smooth=True\n",
    "smooth_filter_cfg='configs/_base_/filters/one_euro.py'\n",
    "device='cuda:0'\n",
    "bbox_thr=0.3\n",
    "kpt_thr=0.5\n",
    "use_oks_tracking=True\n",
    "tracking_thr=0.3\n",
    "euro=0\n",
    "det_cat_id=1\n",
    "show=False\n",
    "\n",
    "\n",
    "print('Initializing model...')\n",
    "det_model = init_detector(\n",
    "    det_config, det_checkpoint, device=device.lower())\n",
    "# build the pose model from a config file and a checkpoint file\n",
    "pose_model = init_pose_model(\n",
    "    pose_config, pose_checkpoint, device=device.lower())\n",
    "\n",
    "dataset = pose_model.cfg.data['test']['type']\n",
    "dataset_info = pose_model.cfg.data['test'].get('dataset_info', None)\n",
    "if dataset_info is None:\n",
    "    warnings.warn(\n",
    "        'Please set `dataset_info` in the config.'\n",
    "        'Check https://github.com/open-mmlab/mmpose/pull/663 for details.',\n",
    "        DeprecationWarning)\n",
    "else:\n",
    "    dataset_info = DatasetInfo(dataset_info)\n",
    "print(dataset_info.skeleton)\n",
    "# read video\n",
    "video = mmcv.VideoReader(vid)\n",
    "assert video.opened, f'Failed to load video file {vid}'\n",
    "\n",
    "if out_vid == '':\n",
    "    save_out_video = False\n",
    "else:\n",
    "    os.makedirs(out_vid, exist_ok=True)\n",
    "    save_out_video = True\n",
    "\n",
    "if save_out_video:\n",
    "    fps = video.fps\n",
    "    size = (video.width, video.height)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    videoWriter = cv2.VideoWriter(\n",
    "        os.path.join(out_vid,\n",
    "                     f'vis_{os.path.basename(vid)}'), fourcc,\n",
    "        fps, size)\n",
    "\n",
    "# frame index offsets for inference, used in multi-frame inference setting\n",
    "if use_multi_frames:\n",
    "    assert 'frame_indices_test' in pose_model.cfg.data.test.data_cfg\n",
    "    indices = pose_model.cfg.data.test.data_cfg['frame_indices_test']\n",
    "\n",
    "# build pose smoother for temporal refinement\n",
    "if euro:\n",
    "    warnings.warn(\n",
    "        'Argument --euro will be deprecated in the future. '\n",
    "        'Please use --smooth to enable temporal smoothing, and '\n",
    "        '--smooth-filter-cfg to set the filter config.',\n",
    "        DeprecationWarning)\n",
    "    smoother = Smoother(\n",
    "        filter_cfg='configs/_base_/filters/one_euro.py', keypoint_dim=2)\n",
    "elif smooth:\n",
    "    smoother = Smoother(filter_cfg=smooth_filter_cfg, keypoint_dim=2)\n",
    "else:\n",
    "    smoother = None\n",
    "\n",
    "# whether to return heatmap, optional\n",
    "return_heatmap = False\n",
    "\n",
    "# return the output of some desired layers,\n",
    "# e.g. use ('backbone', ) to return backbone feature\n",
    "output_layer_names = None\n",
    "\n",
    "next_id = 0\n",
    "pose_results = []\n",
    "print('Running inference...')\n",
    "for frame_id, cur_frame in enumerate(mmcv.track_iter_progress(video)):\n",
    "    pose_results_last = pose_results\n",
    "\n",
    "    # get the detection results of current frame\n",
    "    # the resulting box is (x1, y1, x2, y2)\n",
    "    mmdet_results = inference_detector(det_model, cur_frame)\n",
    "\n",
    "    # keep the person class bounding boxes.\n",
    "    person_results = process_mmdet_results(mmdet_results, det_cat_id)\n",
    "\n",
    "    if use_multi_frames:\n",
    "        frames = collect_multi_frames(video, frame_id, indices,\n",
    "                                      online)\n",
    "\n",
    "    # test a single image, with a list of bboxes.\n",
    "    pose_results, _ = inference_top_down_pose_model(\n",
    "        pose_model,\n",
    "        frames if use_multi_frames else cur_frame,\n",
    "        person_results,\n",
    "        bbox_thr=bbox_thr,\n",
    "        format='xyxy',\n",
    "        dataset=dataset,\n",
    "        dataset_info=dataset_info,\n",
    "        return_heatmap=return_heatmap,\n",
    "        outputs=output_layer_names)\n",
    "\n",
    "    # get track id for each person instance\n",
    "    pose_results, next_id = get_track_id(\n",
    "        pose_results,\n",
    "        pose_results_last,\n",
    "        next_id,\n",
    "        use_oks=use_oks_tracking,\n",
    "        tracking_thr=tracking_thr,\n",
    "        sigmas=dataset_info.sigmas)\n",
    "\n",
    "    # post-process the pose results with smoother\n",
    "    if smoother:\n",
    "        pose_results = smoother.smooth(pose_results)\n",
    "\n",
    "    # show the results\n",
    "    vis_frame = vis_pose_tracking_result(\n",
    "        pose_model,\n",
    "        cur_frame,\n",
    "        pose_results,\n",
    "        radius=radius,\n",
    "        thickness=thickness,\n",
    "        dataset=dataset,\n",
    "        dataset_info=dataset_info,\n",
    "        kpt_score_thr=kpt_thr,\n",
    "        show=False)\n",
    "\n",
    "    if show:\n",
    "        cv2.imshow('Frame', vis_frame)\n",
    "\n",
    "    if save_out_video:\n",
    "        videoWriter.write(vis_frame)\n",
    "\n",
    "    if show and cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "if save_out_video:\n",
    "    videoWriter.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33dfdf47-1f37-4a68-bf73-0dbb1364505f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f54e546-43df-439b-b09f-72f5f24b9b06",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Extract all AcinoSet 3D data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb5f6b8-97f9-48b2-8555-350bde3ec8c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_cameras(n,calib_file,cameras_dict):\n",
    "    calib_dict=mmcv.load(calib_file)\n",
    "    \n",
    "    for i in range(n):\n",
    "        R=np.array(calib_dict['cameras'][i]['r'])\n",
    "        T=np.array(calib_dict['cameras'][i]['t'])\n",
    "        K=np.array(calib_dict['cameras'][i]['k'][:2])\n",
    "        k=np.array(calib_dict['cameras'][i]['d'][:2])\n",
    "        k=np.append(k,[k[1]/2],axis=0)\n",
    "        p=np.array(calib_dict['cameras'][i]['d'][-2:])\n",
    "        w=calib_dict['camera_resolution'][0]\n",
    "        h=calib_dict['camera_resolution'][1]\n",
    "        \n",
    "        date=calib_file.split('/')[2]\n",
    "        \n",
    "        if 'top' in calib_file.split('/'):\n",
    "            name=f'{date}_1_cam{i+1}'\n",
    "            idx=f'{date}_1{i+1}'.replace('_','')\n",
    "        elif 'bottom' in calib_file.split('/'):\n",
    "            name=f'{date}_2_cam{i+1}'\n",
    "            idx=f'{date}_2{i+1}'.replace('_','')\n",
    "        else:\n",
    "            assert calib_file.split('/')[2].split('_')[0]=='2019'\n",
    "            name=f'{date}_cam{i+1}'\n",
    "            idx=f'{date}_0{i+1}'.replace('_','')\n",
    "            \n",
    "        cameras_dict[idx]=dict(R=R,T=T,K=K,k=k,p=p,w=w,h=h,name=name)\n",
    "        cameras_dict[idx]['id']=idx\n",
    "        \n",
    "    return cameras_dict\n",
    "\n",
    "def get_cam(camera_id,cameras):\n",
    "    for item in list(cameras.keys()):\n",
    "        copy=item.replace('_','')\n",
    "        copy=copy.replace('cam','0')\n",
    "        if copy==camera_id:\n",
    "            \n",
    "            return cameras[item]\n",
    "        else:\n",
    "            raise Exception(\"Check camera dict\")\n",
    "\n",
    "def _get_pose_stats(kps):\n",
    "    \"\"\"Get statistic information `mean` and `std` of pose data.\n",
    "    Args:\n",
    "        kps (ndarray): keypoints in shape [..., K, C] where K and C is\n",
    "            the keypoint category number and dimension.\n",
    "    Returns:\n",
    "        mean (ndarray): [K, C]\n",
    "    \"\"\"\n",
    "    assert kps.ndim > 2\n",
    "    K, C = kps.shape[-2:]\n",
    "    kps = kps.reshape(-1, K, C)\n",
    "    mean = kps.mean(axis=0)\n",
    "    std = kps.std(axis=0)\n",
    "    return mean, std\n",
    "\n",
    "import pandas as pd\n",
    "def get_anns(cam_id, cheetah, action, cam, date, start_frame, end_frame):\n",
    "    ''' Extract frames from each camera and get 2D, 3D annotations'''\n",
    "    \n",
    "    #Save frames\n",
    "    imgnames=[]\n",
    "    video=cv2.VideoCapture(f'data/acino_3d/{date}/{cheetah}/{action}/{cam}')\n",
    "    fps=video.get(cv2.CAP_PROP_FPS)\n",
    "    success,image = video.read()\n",
    "    count=0\n",
    "    while success:\n",
    "        if count>=start_frame and count<end_frame:\n",
    "            imgname=f'{cheetah}'+date.split('_')[1]+(date.split('_')[2])+f'_{action}.{cam_id}_{count}.jpg'\n",
    "            if not os.path.exists(f'data/acino_3d/images/{imgname}'):\n",
    "                cv2.imwrite(f'data/acino_3d/images/{imgname}', image)  # save frame as JPEG file      \n",
    "            imgnames.append(imgname)\n",
    "        success,image = video.read()\n",
    "        # print('Read a new frame: ', success)\n",
    "        count += 1\n",
    "    video.release()\n",
    "    imgnames=np.array(imgnames)\n",
    "    \n",
    "    #2D Anns\n",
    "    anns2d=pd.read_csv(f'data/acino_3d/{date}/{cheetah}/{action}/fte_pw/'+cam.split('.')[0]+'_fte.csv')\n",
    "    anns2d.columns=anns2d.columns+anns2d.iloc[0] # Make keypoint labels column headers\n",
    "    anns2d=anns2d.iloc[1:,1:]\n",
    "\n",
    "    # spinel=anns2d.pop('spine.2likelihood')\n",
    "    # anns2d.insert(0,'spinel',spinel)\n",
    "    # spiney=anns2d.pop('spine.1y')\n",
    "    # anns2d.insert(0,'spiney',spiney)\n",
    "    # spinex=anns2d.pop('spinex')\n",
    "    # anns2d.insert(0,'spinex',spinex)\n",
    "\n",
    "    kp2d=[]\n",
    "    for img in range(len(anns2d)):\n",
    "        for kp in range(0,len(anns2d.columns),3):\n",
    "            if math.isnan(float(anns2d.iloc[img,kp])): \n",
    "                kp2d.append(0)\n",
    "                kp2d.append(0)\n",
    "                kp2d.append(bool(0))\n",
    "            else:\n",
    "                kp2d.append(float(anns2d.iloc[img,kp]))           \n",
    "                kp2d.append(float(anns2d.iloc[img,kp+1]))\n",
    "                kp2d.append(bool(1))\n",
    "                \n",
    "    kp2d=np.array(kp2d,dtype=float).reshape(len(anns2d),20,3)\n",
    "    \n",
    "    #3D Anns\n",
    "    anns3d=mmcv.load(f'data/acino_3d/{date}/{cheetah}/{action}/fte_pw/fte.pickle')\n",
    "    # print(len(anns3d['positions']))\n",
    "    # assert len(anns3d['positions'])==20\n",
    "    kp3d=[]\n",
    "    for ann in anns3d['positions']:\n",
    "        # spine=ann[4]\n",
    "        # ann=np.delete(ann,4,axis=0)\n",
    "        # ann=np.insert(ann,0,spine,axis=0)\n",
    "        assert len(ann)==20\n",
    "        kp3d.append(ann)\n",
    "        \n",
    "    kp3d=np.array(kp3d,dtype=float).reshape(len(anns3d['positions']),20,3)\n",
    "    kp3d = np.concatenate([kp3d, np.ones((len(kp3d), 20, 1))],\n",
    "                                axis=2)\n",
    "    \n",
    "    # calculate bounding boxes,centers,scales\n",
    "    scale_factor=1.2\n",
    "    bboxes = np.stack([np.min(kp2d[:, :, 0], axis=1),np.min(kp2d[:, :, 1], axis=1),np.max(kp2d[:, :, 0], axis=1),np.max(kp2d[:, :, 1], axis=1)],axis=1)\n",
    "    centers = np.stack([(bboxes[:, 0] + bboxes[:, 2]) / 2,(bboxes[:, 1] + bboxes[:, 3]) / 2], axis=1)\n",
    "    scales = scale_factor * np.max(bboxes[:, 2:] - bboxes[:, :2], axis=1) / 200\n",
    "    \n",
    "    assert len(imgnames)==kp3d.shape[0]\n",
    "    assert kp3d.shape[0]==kp2d.shape[0]\n",
    "    \n",
    "    return imgnames,centers,scales,kp2d,kp3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc71c05e-0d4d-4f3a-8d43-fb2d1388b1a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Cameras\n",
    "cameras={}\n",
    "for date in os.listdir('data/acino_3d'):\n",
    "    if date.split('_')[0]=='2019':\n",
    "        #Cameras\n",
    "        if osp.exists(f'data/acino_3d/{date}/extrinsic_calib/6_cam_scene.json'):\n",
    "            calib=f'data/acino_3d/{date}/extrinsic_calib/6_cam_scene.json'\n",
    "            n=6 # cam scene\n",
    "            add_cameras(n,calib,cameras)\n",
    "\n",
    "out_file='data/acino_3d/annotations/cameras.pkl'\n",
    "with open(out_file, 'wb') as fout:\n",
    "    pickle.dump(cameras, fout)\n",
    "print(f'Camera parameters have been written to \"{out_file}\".\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853eef6a-cd84-47b9-aa24-ddf2f704a467",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgnames_all=[]\n",
    "centers_all=[]\n",
    "scales_all=[]\n",
    "kps2d_all=[]\n",
    "kps3d_all=[]\n",
    "\n",
    "test_count=0\n",
    "train_count=0\n",
    "mode='train'\n",
    "\n",
    "for date in os.listdir('data/acino_3d'):\n",
    "    \n",
    "    if date.split('_')[0]=='2019':\n",
    "        \n",
    "        #Images\n",
    "        if not osp.exists(f'data/acino_3d/{date}/processed.txt'):\n",
    "            for cheetah in os.listdir(f'data/acino_3d/{date}'):\n",
    "                if cheetah != 'extrinsic_calib':\n",
    "                    # mmcv.mkdir_or_exist(f'{cheetah}')\n",
    "\n",
    "                    for action in os.listdir(f'data/acino_3d/{date}/{cheetah}'):\n",
    "                          \n",
    "                        reconstr_params=mmcv.load(f'data/acino_3d/{date}/{cheetah}/{action}/fte_pw/reconstruction_params.json')\n",
    "                        start_frame=reconstr_params['start_frame']\n",
    "                        end_frame=reconstr_params['end_frame']\n",
    "\n",
    "                        for cam in os.listdir(f'data/acino_3d/{date}/{cheetah}/{action}'):\n",
    "                            \n",
    "                            if cam.endswith('.mp4'):\n",
    "\n",
    "                                cam_name=cam.split('.')[0]\n",
    "                                cam_id=date.replace('_','')+'0'+cam_name[-1]\n",
    "\n",
    "                                print(f'{date}, {mode}, {cheetah}, {action}')\n",
    "                                \n",
    "                                imgnames,centers,scales,kps2d,kps3d = get_anns(cam_id, cheetah, action, cam, date, start_frame, end_frame)\n",
    "                                \n",
    "                                imgnames_all.append(imgnames)\n",
    "                                centers_all.append(centers)\n",
    "                                scales_all.append(scales)\n",
    "                                kps2d_all.append(kps2d)\n",
    "                                kps3d_all.append(kps3d)\n",
    "                                \n",
    "            # with open(f'data/acino_3d/{date}/processed.txt','w') as processed:\n",
    "            #     processed.write(f'{date} processed')\n",
    "\n",
    "        \n",
    "        if mode=='train':\n",
    "            train_count+=1\n",
    "        if mode=='test':\n",
    "            test_count+=1\n",
    "            \n",
    "        if train_count==3:\n",
    "            \n",
    "            imgnames_all = np.concatenate(imgnames_all)\n",
    "            centers_all = np.concatenate(centers_all)\n",
    "            scales_all = np.concatenate(scales_all)\n",
    "            kps2d_all = np.concatenate(kps2d_all)\n",
    "            kps3d_all = np.concatenate(kps3d_all)\n",
    "            np.savez(file=f'data/acino_3d/annotations/acino3d_{mode}.npz',imgname=imgnames_all,center=centers_all,scale=scales_all,part=kps2d_all,S=kps3d_all)\n",
    "            print(f\"Saved: {mode}\")\n",
    "            \n",
    "            if mode=='train':\n",
    "                kps_3d_all = kps3d_all[..., :3]  # remove visibility\n",
    "                mean_3d, std_3d = _get_pose_stats(kps_3d_all)\n",
    "\n",
    "                kps_2d_all = kps2d_all[..., :2]  # remove visibility\n",
    "                mean_2d, std_2d = _get_pose_stats(kps_2d_all)\n",
    "\n",
    "                # centered around root\n",
    "                # the root keypoint is 0-index\n",
    "                kps_3d_rel = kps_3d_all[..., 1:, :] - kps_3d_all[..., :1, :]\n",
    "                mean_3d_rel, std_3d_rel = _get_pose_stats(kps_3d_rel)\n",
    "\n",
    "                kps_2d_rel = kps_2d_all[..., 1:, :] - kps_2d_all[..., :1, :]\n",
    "                mean_2d_rel, std_2d_rel = _get_pose_stats(kps_2d_rel)\n",
    "\n",
    "                stats = {\n",
    "                    'joint3d_stats': {\n",
    "                        'mean': mean_3d,\n",
    "                        'std': std_3d\n",
    "                    },\n",
    "                    'joint2d_stats': {\n",
    "                        'mean': mean_2d,\n",
    "                        'std': std_2d\n",
    "                    },\n",
    "                    'joint3d_rel_stats': {\n",
    "                        'mean': mean_3d_rel,\n",
    "                        'std': std_3d_rel\n",
    "                    },\n",
    "                    'joint2d_rel_stats': {\n",
    "                        'mean': mean_2d_rel,\n",
    "                        'std': std_2d_rel\n",
    "                    }\n",
    "                }\n",
    "\n",
    "                for name, stat_dict in stats.items():\n",
    "                    out_file = f'data/acino_3d/annotations/{name}.pkl'\n",
    "                    with open(out_file, 'wb') as f:\n",
    "                        pickle.dump(stat_dict, f)\n",
    "                    print(f'Create statistic data file: {out_file}')\n",
    "                \n",
    "                train_count=0\n",
    "                \n",
    "                mode='test'\n",
    "                print(\"Test data now. Resetting arrays.\")\n",
    "                imgnames_all=[]\n",
    "                centers_all=[]\n",
    "                scales_all=[]\n",
    "                kps2d_all=[]\n",
    "                kps3d_all=[]\n",
    "            \n",
    "        if test_count==1:\n",
    "            imgnames_all = np.concatenate(imgnames_all)\n",
    "            centers_all = np.concatenate(centers_all)\n",
    "            scales_all = np.concatenate(scales_all)\n",
    "            kps2d_all = np.concatenate(kps2d_all)\n",
    "            kps3d_all = np.concatenate(kps3d_all)\n",
    "            np.savez(file=f'data/acino_3d/annotations/acino3d_{mode}.npz',imgname=imgnames_all,center=centers_all,scale=scales_all,part=kps2d_all,S=kps3d_all)\n",
    "            print(f\"Saved {mode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ffdfd3-f411-436d-92dc-0502bf518106",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgnames_all = np.concatenate(imgnames_all)\n",
    "centers_all = np.concatenate(centers_all)\n",
    "scales_all = np.concatenate(scales_all)\n",
    "kps2d_all = np.concatenate(kps2d_all)\n",
    "kps3d_all = np.concatenate(kps3d_all)\n",
    "np.savez(file=f'data/acino_3d/annotations/acino3d_{mode}.npz',imgname=imgnames_all,center=centers_all,scale=scales_all,part=kps2d_all,S=kps3d_all)\n",
    "print(f\"Saved {mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1b64e-65fb-404f-9ac1-8bef5092e930",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Train PoseLifter model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e44fc0-e0e3-4e18-9104-a0ea741ed461",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cfg = Config.fromfile('configs/body/3d_kpt_sview_rgb_vid/video_pose_lift/h36m/videopose3d_acino_27frames_fullconv_supervised.py')\n",
    "# For new configs:\n",
    "\n",
    "cfg.work_dir='work_dirs/videopose3d_acino_27frames_epoch160'    \n",
    "# create work_dir\n",
    "mmcv.mkdir_or_exist(cfg.work_dir)\n",
    "                                  \n",
    "# Config Parameters:\n",
    "cfg.total_epochs=800\n",
    "\n",
    "cfg.resume_from=None # the checkpoint file to resume from\n",
    "\n",
    "cfg.gpu_ids=range(1)\n",
    "cfg.workflow=[('train',1)]\n",
    "\n",
    "cfg.log_name = time.strftime('%d-%m-', time.localtime())+\"ep\"+str(cfg.total_epochs)\n",
    "cfg.log_file = os.path.join(cfg.work_dir, f'{cfg.log_name}.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45765a3a-52c9-4b16-b7ee-33750714a8c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train from config file\n",
    "autoscale_lr = False # automatically scale lr with the number of gpus\n",
    "launcher = 'none' # Job launcher. ['none', 'pytorch', 'slurm', 'mpi']\n",
    "deterministic = False # Whether to set deterministic options for CUDNN backend.\n",
    "diff_seed = False # Whether or not set different seeds for different ranks\n",
    "seed = 0 \n",
    "\n",
    "if autoscale_lr:\n",
    "    # apply the linear scaling rule (https://arxiv.org/abs/1706.02677)\n",
    "    cfg.optimizer['lr'] = cfg.optimizer['lr'] * len(cfg.gpu_ids) / 8\n",
    "\n",
    "if launcher == 'none':\n",
    "    distributed = False\n",
    "    if len(cfg.gpu_ids) > 1:\n",
    "        warnings.warn(\n",
    "            f'We treat {cfg.gpu_ids} as gpu-ids, and reset to '\n",
    "            f'{cfg.gpu_ids[0:1]} as gpu-ids to avoid potential error in '\n",
    "            'non-distribute training time.')\n",
    "        cfg.gpu_ids = cfg.gpu_ids[0:1]\n",
    "else:\n",
    "    distributed = True\n",
    "    init_dist(launcher, **cfg.dist_params)\n",
    "    # re-set gpu_ids with distributed training mode\n",
    "    _, world_size = get_dist_info()\n",
    "    cfg.gpu_ids = range(world_size)\n",
    "\n",
    "# set multi-process settings\n",
    "setup_multi_processes(cfg)\n",
    "\n",
    "# init the logger before other steps\n",
    "logger = get_root_logger(log_file=cfg.log_file, log_level=cfg.log_level)\n",
    "\n",
    "# init the meta dict to record some important information such as\n",
    "# environment info and seed, which will be logged\n",
    "meta = dict()\n",
    "\n",
    "# log env info\n",
    "env_info_dict = collect_env()\n",
    "env_info = '\\n'.join([(f'{k}: {v}') for k, v in env_info_dict.items()])\n",
    "dash_line = '-' * 60 + '\\n'\n",
    "# logger.info('Environment info:\\n' + dash_line + env_info + '\\n' +\n",
    "#             dash_line)\n",
    "meta['env_info'] = env_info\n",
    "\n",
    "# log some basic info\n",
    "logger.info(f'Distributed training: {distributed}')\n",
    "# logger.info(f'Config:\\n{cfg.pretty_text}')\n",
    "\n",
    "# set random seeds\n",
    "seed = init_random_seed(seed)\n",
    "seed = seed + dist.get_rank() if diff_seed else seed\n",
    "logger.info(f'Set random seed to {seed}, '\n",
    "            f'deterministic: {deterministic}')\n",
    "set_random_seed(seed, deterministic=deterministic)\n",
    "cfg.seed = seed\n",
    "meta['seed'] = seed\n",
    "\n",
    "# build dataset\n",
    "train_set = [build_dataset(cfg.data.train)]\n",
    "\n",
    "# build model\n",
    "model = build_posenet(cfg.model)\n",
    "\n",
    "if len(cfg.workflow) == 2:\n",
    "    val_dataset = copy.deepcopy(cfg.data.val)\n",
    "    val_dataset.pipeline = cfg.data.train.pipeline\n",
    "    train_set.append(build_dataset(val_dataset))\n",
    "\n",
    "if cfg.checkpoint_config is not None:\n",
    "    # save mmpose version, config file content\n",
    "    # checkpoints as meta data\n",
    "    cfg.checkpoint_config.meta = dict(\n",
    "        mmpose_version=__version__ + get_git_hash(digits=7),\n",
    "        # config=cfg.pretty_text,\n",
    "    )\n",
    "\n",
    "timestamp = time.strftime('%d-%m-', time.localtime())+\"ep\"+str(cfg.total_epochs)\n",
    "print(\"LINE\")\n",
    "# train model\n",
    "train_model(\n",
    "    model,\n",
    "    train_set, \n",
    "    cfg, \n",
    "    distributed=distributed, \n",
    "    validate=True, \n",
    "    timestamp=timestamp,\n",
    "    meta=meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ea2be4-e9ab-4390-8bcb-f99af8c8a983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python demo/body3d_two_stage_video_demo.py MMDet/configs/faster_rcnn_r50_fpn_2x_acino.py MMDet/epoch_4.pth configs/animal/2d_kpt_sview_rgb_img/topdown_heatmap/acino/res50_acino_256x256.py work_dirs/res50_acino_256x256_epoch150/.pth configs/body/3d_kpt_sview_rgb_vid/video_pose_lift/h36m/videopose3d_acino_27frames_fullconv_supervised.py work_dirs/videopose3d_acino_27frames_epoch160/latest.pth --video-path cam3ph.mp4 --out-video-root cam3ph3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72b3ac5-7063-406b-9ec3-deb2d4227a54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test=np.load('data/acino_3d/annotations/acino3d_test.npz')\n",
    "train=np.load('data/acino_3d/annotations/acino3d_train.npz')\n",
    "all_centers=np.concatenate((test['center'],train['center']),axis=0)\n",
    "center_avg=np.average(all_centers,axis=0)\n",
    "print(center_avg)\n",
    "\n",
    "all_scales=np.concatenate((test['scale'],train['scale']),axis=0)\n",
    "# print(all_scales)\n",
    "scale_avg=np.average(all_scales,axis=0)\n",
    "scale_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212a2d8-86e6-4dce-b2fb-41e8b27b51c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "person_det_model = init_detector('MMDet/configs/faster_rcnn_r50_fpn_2x_acino.py', 'MMDet/epoch_4.pth')\n",
    "pose_det_model=init_pose_model('configs/animal/2d_kpt_sview_rgb_img/topdown_heatmap/acino/res50_acino_256x256.py','work_dirs/res50_acino_256x256_epoch150/latest.pth')\n",
    "\n",
    "pose_det_dataset = pose_det_model.cfg.data['test']['type']\n",
    "# get datasetinfo\n",
    "dataset_info = pose_det_model.cfg.data['test'].get('dataset_info', None)\n",
    "dataset_info=DatasetInfo(dataset_info)\n",
    "# print(dataset_info.skeleton)\n",
    "\n",
    "video = mmcv.VideoReader('jules0305/runcam1.mp4')\n",
    "\n",
    "return_heatmap=False\n",
    "output_layer_names = None\n",
    "use_multi_frames=False\n",
    "rebase_keypoint_height=True\n",
    "num_instances=-1\n",
    "pose_det_results_list = []\n",
    "next_id = 0\n",
    "pose_det_results = []\n",
    "save_out_video=True\n",
    "\n",
    "print(\"\\n2D Detection and Pose Estimation:\")\n",
    "for frame_id, cur_frame in enumerate(mmcv.track_iter_progress(video)):\n",
    "    \n",
    "    pose_det_results_last = pose_det_results\n",
    "    \n",
    "    # get the detection results of current frame\n",
    "    # the resulting box is (x1, y1, x2, y2)\n",
    "    mmdet_results = inference_detector(person_det_model, cur_frame)\n",
    "\n",
    "    # keep the person class bounding boxes.\n",
    "    person_det_results = process_mmdet_results(mmdet_results, 1)\n",
    "\n",
    "    # if use_multi_frames:\n",
    "    #     frames = collect_multi_frames(video, frame_id, indices,\n",
    "    #                                   args.online)\n",
    "\n",
    "    # test a single image, with a list of bboxes.\n",
    "    pose_det_results, _ = inference_top_down_pose_model(\n",
    "        pose_det_model,\n",
    "        cur_frame,\n",
    "        person_det_results,\n",
    "        format='xyxy',\n",
    "        dataset=pose_det_dataset,\n",
    "        dataset_info=dataset_info,\n",
    "        return_heatmap=return_heatmap,\n",
    "        outputs=output_layer_names)\n",
    "    \n",
    "    pose_det_results, next_id = get_track_id(\n",
    "            pose_det_results,\n",
    "            pose_det_results_last,\n",
    "            next_id)\n",
    "    \n",
    "    pose_det_results_list.append(copy.deepcopy(pose_det_results))\n",
    "\n",
    "    # show the results\n",
    "    # if frame_id%20==0:\n",
    "    #     out=f'menyacam3/{frame_id}.jpg'  # save frame as JPEG file\n",
    "    #     # print(pose_det_results)\n",
    "    # else:\n",
    "    #     out=None\n",
    "      \n",
    "    vis_frame = vis_pose_result(\n",
    "        pose_det_model,\n",
    "        cur_frame,\n",
    "        pose_det_results,\n",
    "        dataset=pose_det_dataset,\n",
    "        dataset_info=dataset_info,\n",
    "        show=False,\n",
    "        out_file=None)\n",
    "        # kpt_score_thr=0.003\n",
    "    \n",
    "pose_lift_model=init_pose_model('configs/body/3d_kpt_sview_rgb_vid/video_pose_lift/h36m/videopose3d_acino_27frames_fullconv_supervised.py','work_dirs/videopose3d_acino_27frames_epoch160/latest.pth')\n",
    "pose_lift_dataset = pose_lift_model.cfg.data['test']['type']\n",
    "\n",
    "if save_out_video:\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    fps = video.fps\n",
    "    writer = None\n",
    "\n",
    "for pose_det_results in pose_det_results_list:\n",
    "    for res in pose_det_results:\n",
    "        keypoints = res['keypoints']\n",
    "        keypoints_new = np.zeros((20, keypoints.shape[1]), dtype=keypoints.dtype)\n",
    "        keypoints_new=keypoints[[2,1,0,3,\n",
    "                                    23,4,18,17,\n",
    "                                    8,9,19,\n",
    "                                    5,6,20,\n",
    "                                    14,15,21,\n",
    "                                    11,12,22]] \n",
    "        res['keypoints']=keypoints_new\n",
    "        \n",
    "if hasattr(pose_lift_model.cfg, 'test_data_cfg'):\n",
    "    data_cfg = pose_lift_model.cfg.test_data_cfg\n",
    "else:\n",
    "    data_cfg = pose_lift_model.cfg.data_cfg\n",
    "\n",
    "smoother=Smoother('configs/_base_/filters/one_euro.py',keypoint_key='keypoints',keypoint_dim=2)\n",
    "# if i==80:\n",
    "#         print(pose_results_2d)\n",
    "#         print()\n",
    "        \n",
    "#         print(pose_lift_results[0])\n",
    "pose_lift_dataset_info = pose_lift_model.cfg.data['test'].get('dataset_info', None)\n",
    "pose_lift_dataset_info = DatasetInfo(pose_lift_dataset_info)\n",
    "\n",
    "print(\"\\nPose Lifting:\")\n",
    "for i, pose_det_results in enumerate(mmcv.track_iter_progress(pose_det_results_list)): #Pose det results: iterate over each frame\n",
    "    # extract and pad input pose2d sequence\n",
    "    pose_results_2d = extract_pose_sequence( # returns 2D kp and BBox for 27 frames. len = 27\n",
    "        pose_det_results_list,\n",
    "        frame_idx=i,\n",
    "        causal=data_cfg.causal,\n",
    "        seq_len=data_cfg.seq_len,\n",
    "        step=data_cfg.seq_frame_interval)\n",
    "\n",
    "    # # smooth 2d results\n",
    "    # if smoother:\n",
    "    #     pose_results_2d = smoother.smooth(pose_results_2d)\n",
    "\n",
    "    # 2D-to-3D pose lifting\n",
    "    pose_lift_results = inference_pose_lifter_model( #'keypoints': 2D keypoints for 27 frames, 'keypoints_3d': 3D keypoints for target frame\n",
    "        pose_lift_model,\n",
    "        pose_results_2d=pose_results_2d,\n",
    "        dataset=pose_lift_dataset,\n",
    "        dataset_info=pose_lift_dataset_info,\n",
    "        with_track_id=True,\n",
    "        image_size=video.resolution,\n",
    "        norm_pose_2d=False)\n",
    "    # print(pose_lift_results) #Norm = true\n",
    "    \n",
    "              \n",
    "    pose_lift_results_vis = []\n",
    "    for idx, res in enumerate(pose_lift_results):\n",
    "        keypoints_3d = res['keypoints_3d']\n",
    "        # exchange y,z-axis, and then reverse the direction of x,z-axis\n",
    "        # keypoints_3d = keypoints_3d[..., [0, 2, 1]]\n",
    "        # keypoints_3d[..., 0] = -keypoints_3d[..., 0]\n",
    "        # keypoints_3d[..., 2] = -keypoints_3d[..., 2]\n",
    "    #     # rebase height (z-axis)\n",
    "        if rebase_keypoint_height:\n",
    "            keypoints_3d[..., 2] -= np.min(\n",
    "                keypoints_3d[..., 2], axis=-1, keepdims=True)\n",
    "        res['keypoints_3d'] = keypoints_3d\n",
    "    #     # add title\n",
    "        det_res = pose_det_results[idx]\n",
    "        instance_id = det_res['track_id']\n",
    "        res['title'] = f'Prediction ({instance_id})'\n",
    "    #     # only visualize the target frame\n",
    "        res['keypoints'] = det_res['keypoints']\n",
    "        res['bbox'] = det_res['bbox']\n",
    "        res['track_id'] = instance_id\n",
    "        pose_lift_results_vis.append(res)\n",
    "    \n",
    "    # print(len(pose_lift_results_vis))\n",
    "    # Visualization\n",
    "    \n",
    "    if len(pose_lift_results_vis)==1:\n",
    "\n",
    "        if num_instances < 0:\n",
    "            num_instances = len(pose_lift_results_vis)\n",
    "        # if i==80:\n",
    "        #     out='m/frame80.jpg'\n",
    "        #     print(pose_lift_results_vis)\n",
    "        # else:\n",
    "        #     out=None\n",
    "            \n",
    "        img_vis = vis_3d_pose_result(\n",
    "            pose_lift_model,\n",
    "            result=pose_lift_results_vis,\n",
    "            img=video[i],\n",
    "            dataset=pose_lift_dataset,\n",
    "            dataset_info=pose_lift_dataset_info,\n",
    "            out_file=None,\n",
    "            num_instances=num_instances,\n",
    "            show=False)\n",
    "\n",
    "        if save_out_video:\n",
    "            if writer is None:\n",
    "                writer = cv2.VideoWriter(\n",
    "                    osp.join('jules0305',\n",
    "                             'runcam1_out.mp4'), fourcc,\n",
    "                    fps, (img_vis.shape[1], img_vis.shape[0]))\n",
    "            writer.write(img_vis)\n",
    "\n",
    "if save_out_video:\n",
    "    writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3cda0-4982-4865-97c9-84d9e149bf0b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pose_lift_results_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d46bb9-6437-47e6-85a2-8c8a2775cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "menya3d['positions'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda59bd4-fd1f-4eeb-9f5d-b79b024587e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pose_lift_results[0]['keypoints_3d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a0e91-54fa-4fb5-b237-8789407081a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(pose_lift_results[0]['bbox']) #Norm = true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb141738-cdfb-4810-a7da-7d53b3e89e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "menya3d=mmcv.load('menyacam3/fte.pickle')\n",
    "# print(menya3d['positions'][0])\n",
    "print(menya3d['positions'][80-69])\n",
    "kp3d=[]\n",
    "for ann in menya3d['positions']:\n",
    "    spine=ann[4]\n",
    "    ann=np.delete(ann,4,axis=0)\n",
    "    ann=np.insert(ann,0,spine,axis=0)\n",
    "    kp3d.append(ann)\n",
    "\n",
    "kp3d=np.array(kp3d,dtype=float).reshape(len(menya3d['positions']),20,3)\n",
    "kp3d = np.concatenate([kp3d, np.ones((len(kp3d), 20, 1))],\n",
    "                                axis=2)\n",
    "# print(kp3d[80-69])\n",
    "# kp3d = kp3d[..., [0, 2, 1]]\n",
    "# kp3d[..., 0] = -kp3d[..., 0]\n",
    "# kp3d[..., 2] = -kp3d[..., 2]\n",
    "# print(menya3d['positions']) # [0] == frame 69 | [11]==frame80\n",
    "\n",
    "print(kp3d[80-69])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f7f470-22fa-4530-844a-380a4a66b75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pose_lift_dataset_info.skeleton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f81c54-ef11-49a2-bc9a-f017eb04b864",
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_lift_model=init_pose_model('configs/body/3d_kpt_sview_rgb_vid/video_pose_lift/h36m/videopose3d_acino_27frames_fullconv_supervised.py','work_dirs/videopose3d_acino_27frames_epoch160/latest.pth')\n",
    "pose_lift_dataset = pose_lift_model.cfg.data['test']['type']\n",
    "pose_lift_dataset_info = pose_lift_model.cfg.data['test'].get('dataset_info', None)\n",
    "pose_lift_dataset_info=DatasetInfo(pose_lift_dataset_info)\n",
    "\n",
    "menya3d=mmcv.load('menyacam3/fte.pickle')\n",
    "joints=menya3d['positions'][11]\n",
    "root_idx=4\n",
    "\n",
    "root = joints[..., root_idx:root_idx + 1, :]\n",
    "joints = joints - root\n",
    "# print(joints)\n",
    "joints = joints[..., [0, 2, 1]]\n",
    "joints[..., 0] = -joints[..., 0]\n",
    "joints[..., 2] = -joints[..., 2]\n",
    "\n",
    "joints=[joints]\n",
    "# joints=np.concatenate([joints, np.ones((len(joints), 20, 1))],\n",
    "#                                 axis=2)\n",
    "\n",
    "result=dict(keypoints_3d=joints[0],track_id=int(0))\n",
    "result=[result]\n",
    "vis_3d_pose_result(\n",
    "            pose_lift_model,\n",
    "            result=result,\n",
    "            img=None,#video[i],\n",
    "            dataset=pose_lift_dataset,\n",
    "            dataset_info=pose_lift_dataset_info,\n",
    "            out_file='menyacam3/gt.jpg',\n",
    "            num_instances=-1,\n",
    "            show=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd88192-808e-441d-909d-124db170de35",
   "metadata": {},
   "outputs": [],
   "source": [
    "menya3d=load_pickle('menyacam3/fte.pickle')\n",
    "menya3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85621883-21b7-4c0c-bc66-b824dd2399ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pickle(pickle_file):\n",
    "    \"\"\"\n",
    "    Loads a dictionary from a saved skeleton .pickle file\n",
    "    \"\"\"\n",
    "    with open(pickle_file, 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc7591b-70a0-48e6-80af-4b29f5a3e44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera=mmcv.load('data/acino_3d/annotations/cameras.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e64d94e-78ea-4293-8c9c-99e8783dae0e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data settings\n",
    "data_root = 'data/acino_3d'\n",
    "data_cfg = dict(\n",
    "    num_joints=20,\n",
    "    seq_len=27,\n",
    "    seq_frame_interval=1,\n",
    "    causal=False,\n",
    "    temporal_padding=True,\n",
    "    joint_2d_src='gt',\n",
    "    need_camera_param=True,\n",
    "    camera_param_file=f'{data_root}/annotations/cameras.pkl',\n",
    ")\n",
    "\n",
    "pose_lift_model=init_pose_model('configs/body/3d_kpt_sview_rgb_vid/video_pose_lift/h36m/videopose3d_acino_27frames_fullconv_supervised.py','work_dirs/videopose3d_acino_27frames_epoch160/latest.pth')\n",
    "pose_lift_dataset = pose_lift_model.cfg.data['test']['type']\n",
    "pose_lift_dataset_info = pose_lift_model.cfg.data['test'].get('dataset_info', None)\n",
    "pose_lift_dataset_info=DatasetInfo(pose_lift_dataset_info)\n",
    "data=np.load('data/acino_3d/annotations/acino3d_train.npz')\n",
    "if 'scale' in data:\n",
    "    _scales = data['scale'].astype(np.float32)\n",
    "else:\n",
    "    _scales = np.zeros(num_imgs, dtype=np.float32)\n",
    "\n",
    "if 'center' in data:\n",
    "    _centers = data['center'].astype(np.float32)\n",
    "else:\n",
    "    _centers = np.zeros((num_imgs, 2), dtype=np.float32)\n",
    "\n",
    "# get 3D pose\n",
    "if 'S' in data.keys():\n",
    "    _joints_3d = data['S'].astype(np.float32)\n",
    "else:\n",
    "    _joints_3d = np.zeros((num_imgs, num_joints, 4), dtype=np.float32)\n",
    "\n",
    "# get 2D pose\n",
    "if 'part' in data.keys():\n",
    "    _joints_2d = data['part'].astype(np.float32)\n",
    "else:\n",
    "    _joints_2d = np.zeros((num_imgs, num_joints, 3), dtype=np.float32)\n",
    "\n",
    "data_info = {\n",
    "    'imgnames': _imgnames,\n",
    "    'joints_3d': _joints_3d,\n",
    "    'joints_2d': _joints_2d,\n",
    "    'scales': _scales,\n",
    "    'centers': _centers,\n",
    "}\n",
    "\n",
    "_imgnames = data['imgname']\n",
    "num_imgs = len(_imgnames)\n",
    "num_joints=data_cfg['num_joints']\n",
    "\n",
    "joint_2d_src = data_cfg.get('joint_2d_src', 'gt')\n",
    "# if joint_2d_src not in self.SUPPORTED_JOINT_2D_SRC:\n",
    "#     raise ValueError(\n",
    "#         f'Unsupported joint_2d_src \"{self.joint_2d_src}\". '\n",
    "#         f'Supported options are {self.SUPPORTED_JOINT_2D_SRC}')\n",
    "\n",
    "joint_2d_det_file = data_cfg.get('joint_2d_det_file', None)\n",
    "\n",
    "need_camera_param = data_cfg.get('need_camera_param', False)\n",
    "\n",
    "actions = data_cfg.get('actions', '_all_')\n",
    "actions = set(actions if isinstance(actions, (list, tuple)) else [actions])\n",
    "subjects = data_cfg.get('subjects', '_all_')\n",
    "\n",
    "def _parse_h36m_imgname(imgname):\n",
    "    subj, rest = osp.basename(imgname).split('_', 1)\n",
    "    action, rest = rest.split('.', 1)\n",
    "    camera, rest = rest.split('_', 1)\n",
    "    return subj,action,camera\n",
    "print(action)\n",
    "print(subj)\n",
    "print(camera)\n",
    "\n",
    "video_frames = defaultdict(list)\n",
    "for idx, imgname in enumerate(data_info['imgnames']):\n",
    "    subj, action, camera = _parse_h36m_imgname(imgname)\n",
    "\n",
    "    if '_all_' not in actions and action not in actions:\n",
    "        continue\n",
    "\n",
    "    if '_all_' not in subjects and subj not in subjects:\n",
    "        continue\n",
    "\n",
    "    video_frames[(subj, action, camera)].append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b842b3e-083f-461c-a6c4-d6d237d4fcd8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build sample indices\n",
    "seq_frame_interval=1\n",
    "seq_len=27\n",
    "subset=1\n",
    "sample_indices = []\n",
    "_len = (seq_len - 1) * seq_frame_interval + 1\n",
    "_step = seq_frame_interval\n",
    "for _, _indices in sorted(video_frames.items()):\n",
    "    n_frame = len(_indices)\n",
    "\n",
    "            # Pad the sequence so that every frame in the sequence will be\n",
    "    # predicted.\n",
    "    frames_left = (seq_len - 1) // 2\n",
    "    frames_right = frames_left\n",
    "    for i in range(n_frame):\n",
    "        pad_left = max(0, frames_left - i // _step)\n",
    "        pad_right = max(0,\n",
    "                        frames_right - (n_frame - 1 - i) // _step)\n",
    "        start = max(i % _step, i - frames_left * _step)\n",
    "        end = min(n_frame - (n_frame - 1 - i) % _step,\n",
    "                  i + frames_right * _step + 1)\n",
    "        sample_indices.append([_indices[0]] * pad_left +\n",
    "                              _indices[start:end:_step] +\n",
    "                              [_indices[-1]] * pad_right)\n",
    " \n",
    "\n",
    "# reduce dataset size if self.subset < 1\n",
    "assert 0 < subset <= 1\n",
    "subset_size = int(len(sample_indices) *subset)\n",
    "start = np.random.randint(0, len(sample_indices) - subset_size + 1)\n",
    "end = start + subset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c80c4-e25d-4b59-8cd8-f5173a1cbea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_indices[start:end][50]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
